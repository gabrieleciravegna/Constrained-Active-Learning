{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TorchVision 0.3 Object Detection finetuning tutorial\n",
    "\n",
    "For this tutorial, we will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "First, we need to install `pycocotools`. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pycocotools\n",
    "\n",
    "from vision_utils.engine import train_one_epoch, evaluate\n",
    "import vision_utils.utils as utils\n",
    "import vision_utils.transforms as T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "The [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
    "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return:\n",
    "\n",
    "* image: a PIL Image of size (H, W)\n",
    "* target: a dict containing the following fields\n",
    "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
    "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
    "    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n",
    "    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references/detection/transforms.py` for your new keypoint representation\n",
    "\n",
    "If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Writing a custom dataset for Penn-Fudan\n",
    "\n",
    "Let's write a dataset for the Penn-Fudan dataset.\n",
    "\n",
    "First, let's download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the dataset and how it is layed down.\n",
    "\n",
    "The data is structured as follows\n",
    "```\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "```\n",
    "\n",
    "Here is one example of an image in the dataset, with its corresponding instance segmentation mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "im = Image.open('data/PennFudanPed/PNGImages/FudanPed00001.png')\n",
    "# np.asarray(im).shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.PngImagePlugin.PngImageFile image mode=P size=559x536 at 0x28A97C974C8>",
      "image/png": "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\n"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = Image.open('data/PennFudanPed/PedMasks/FudanPed00001_mask.png')\n",
    "np_mask = np.array(mask)\n",
    "# each mask instance has a different color, from zero to N, where\n",
    "# N is the number of instances. In order to make visualization easier,\n",
    "# let's adda color palette to the mask.\n",
    "mask.putpalette([\n",
    "    0, 0, 0, # black background\n",
    "    255, 0, 0, # index 1 is red\n",
    "    0, 0, 255, # index 2 is yellow\n",
    "    255, 153, 0, # index 3 is orange\n",
    "])\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2], dtype=uint8)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of instances present in each image is given by the numbers of unique id assigned to each pixel (with 0 being the background)\n",
    "np.unique(np.asarray(mask))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's all for the dataset. Let's see how the outputs are structured for this dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(<PIL.Image.Image image mode=RGB size=559x536 at 0x28A97CB3488>,\n {'boxes': tensor([[159., 181., 301., 430.],\n          [419., 170., 534., 485.]]),\n  'labels': tensor([1, 1]),\n  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]],\n  \n          [[0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n  'image_id': tensor([0]),\n  'area': tensor([35358., 36225.]),\n  'iscrowd': tensor([0, 0])})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PennFudanDataset('data/PennFudanPed/')\n",
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
    "containing several fields, including `boxes`, `labels` and `masks`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining your model\n",
    "\n",
    "In this tutorial, we will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
    "\n",
    "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)\n",
    "\n",
    "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n",
    "\n",
    "![Mask R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image04.png)\n",
    "\n",
    "There are two common situations where one might want to modify one of the available models in torchvision modelzoo.\n",
    "The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
    "\n",
    "Let's go see how we would do one or another in the following sections.\n",
    "\n",
    "\n",
    "### 1 - Finetuning from a pretrained model\n",
    "\n",
    "Let's suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:\n",
    "```\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    # model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n",
    "\n",
    "## Training and evaluation functions\n",
    "\n",
    "In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n",
    "Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
    "\n",
    "Let's copy those files (and their dependencies) in here so that they are available in the notebook\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's write some helper functions for data augmentation / transformation, which leverages the functions in `refereces/detection` that we have just copied:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Putting everything together\n",
    "\n",
    "We now have the dataset class, the models and the data transforms. Let's instantiate them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:10])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[10:20])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's instantiate the model and the optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# data = next(iter(data_loader))[0][0]\n",
    "# data = torch.unsqueeze(data, dim=0)\n",
    "# # data\n",
    "# extracted_feat = model.backbone(data)\n",
    "# extracted_feat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/5]  eta: 0:02:03  lr: 0.001254  loss: 1.4965 (1.4965)  loss_classifier: 1.1500 (1.1500)  loss_box_reg: 0.3268 (0.3268)  loss_objectness: 0.0101 (0.0101)  loss_rpn_box_reg: 0.0097 (0.0097)  time: 24.7217  data: 0.1360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-933760c6d5cd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;31m# train for one epoch, printing every 10 iterations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mtrain_one_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprint_freq\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[1;31m# update the learning rate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mlr_scheduler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\Scuola\\Dottorato\\Codice\\Constrained-Active-Learning\\vision_utils\\engine.py\u001B[0m in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, optimizer, data_loader, device, epoch, print_freq, verbose)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     49\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 50\u001B[1;33m         \u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     51\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m    130\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 132\u001B[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    133\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that training has finished, let's have a look at what it actually predicts in a test image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "img2, _ = dataset_test[1]\n",
    "imgs = [img.to(device), img2.to(device)]\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model(imgs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
    "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'boxes': tensor([[371.2947, 203.0686, 401.0109, 239.3348],\n          [161.3843, 204.5503, 170.8735, 245.3437],\n          [112.1572, 203.8908, 128.9252, 274.2334],\n          [370.2305, 214.4206, 399.2830, 253.5218],\n          [118.6764, 202.0528, 136.7223, 275.7271],\n          [377.0554, 197.8059, 410.8587, 228.7941],\n          [231.1873,  94.1928, 254.4865, 141.6436],\n          [168.2643, 187.7844, 179.5891, 256.3888],\n          [164.3980, 192.8048, 175.9299, 252.5683],\n          [147.3093,  72.6699, 214.0782, 299.9202],\n          [216.2527,  85.3400, 235.8354, 147.8322],\n          [371.2840, 182.1978, 433.4570, 255.4221],\n          [ 22.2747,  97.8492,  89.2891, 313.8382],\n          [159.1420, 177.1005, 174.5614, 249.1656],\n          [241.1368,  87.6418, 254.8352, 136.5751],\n          [411.2848, 126.3601, 551.4260, 247.1779],\n          [356.6471, 189.6604, 395.7914, 283.6785],\n          [230.3671, 130.0309, 246.2344, 138.7248],\n          [362.2488, 176.9646, 427.7269, 315.7921],\n          [155.6574, 161.0532, 180.6845, 289.8852],\n          [224.1486,  84.6556, 247.8455, 136.5912],\n          [391.2872, 149.2981, 509.4984, 245.1132],\n          [254.8450, 118.5106, 347.2617, 320.4812],\n          [228.8358, 132.0393, 247.6467, 142.6587],\n          [367.1762, 200.9541, 403.9863, 222.0410],\n          [529.6252, 222.1436, 573.3435, 307.0255],\n          [232.4984, 125.8980, 248.9986, 139.6533],\n          [117.3537, 134.5104, 142.1097, 289.0059],\n          [156.6471,   1.5134, 229.7624,  35.2247],\n          [362.5970, 206.3191, 410.2776, 272.7973],\n          [158.8795, 111.8981, 185.3570, 266.9624],\n          [225.9694,  76.7337, 255.5264, 153.2378],\n          [377.5280, 206.6860, 413.0720, 247.7135],\n          [138.6866, 119.6468, 243.8766, 276.5144],\n          [408.2774, 164.3214, 568.5151, 308.1530],\n          [356.6433, 199.0074, 424.2473, 239.7013],\n          [ 10.0104, 167.8613, 138.0336, 320.3543],\n          [228.7480, 121.4367, 254.3936, 144.4770],\n          [467.0070, 215.9520, 570.9562, 319.3244],\n          [156.6513, 203.9672, 182.2273, 242.4500],\n          [199.2750,  88.3915, 216.3531, 126.3778],\n          [147.5657,  98.4178, 167.9903, 146.0610],\n          [457.9291, 229.3763, 537.8513, 294.1599],\n          [205.9888,  89.8160, 240.2545, 240.5467],\n          [380.3179,  49.5790, 545.9979, 301.5312],\n          [243.7811, 122.3959, 251.4797, 137.0285],\n          [316.2832,  96.5211, 348.1142, 202.5925],\n          [165.6369,  88.2699, 197.9447, 212.1934],\n          [183.8865,   0.0000, 251.9214,  55.6236],\n          [225.9833,  41.0913, 233.6633,  54.6472],\n          [480.9908,  44.8700, 571.4537, 354.9809],\n          [202.2530,  84.3066, 229.7115, 196.0237],\n          [278.5626, 185.5135, 365.0153, 332.1990],\n          [206.9665,  81.0699, 219.0805, 127.4179],\n          [240.1633, 124.4579, 248.7079, 137.1164],\n          [359.5868, 130.7232, 571.5356, 351.3031],\n          [164.9930,  10.4329, 213.3439,  31.0489],\n          [287.8618,  94.2986, 347.1938, 157.6330],\n          [273.6800,  84.6135, 294.4656, 128.8270],\n          [238.8884, 132.6906, 248.5498, 138.0451],\n          [136.6074, 137.3708, 159.8745, 281.7211],\n          [236.8040, 133.6771, 250.2733, 140.5251],\n          [ 76.3167, 103.3684,  84.2250, 115.8911],\n          [  0.0000, 327.4988, 311.2513, 359.5439],\n          [231.0086, 123.8700, 241.1690, 138.3098],\n          [238.0462, 129.8228, 246.9502, 138.6010],\n          [175.7755,  65.1486, 192.9641,  72.9630],\n          [390.7221, 203.3669, 433.2314, 271.0139],\n          [157.5643,   7.7284, 202.3694,  69.4238],\n          [150.1825, 125.1460, 174.5872, 266.8706],\n          [256.2354, 188.7802, 381.1230, 224.4694],\n          [183.8983, 156.5648, 228.0171, 323.4889],\n          [551.9687, 214.6618, 574.7895, 340.1110],\n          [226.2844, 105.8975, 249.0185, 145.1551],\n          [203.1274, 144.5034, 237.9566, 314.4449],\n          [259.8877, 203.0914, 388.6707, 239.1881],\n          [148.6522,  77.3641, 184.2260, 147.2481],\n          [231.8678, 128.3290, 242.3178, 134.8028],\n          [219.2542, 130.7597, 257.6574, 146.1012],\n          [ 36.6624, 198.4080,  48.9169, 207.6219],\n          [303.7545, 317.4863, 320.1712, 326.5092],\n          [191.2313,  36.9241, 236.9345,  57.8005],\n          [ 76.8179, 105.6908,  86.0016, 124.8738],\n          [214.5161,  97.7174, 257.6478, 146.8738],\n          [434.1163, 243.3705, 566.5637, 356.0422],\n          [227.3738, 131.1899, 235.7780, 145.1220],\n          [226.4083, 128.1099, 242.0034, 143.1536],\n          [526.7319, 172.7877, 573.9730, 355.6504],\n          [  0.0000, 269.8477, 269.5860, 329.0374],\n          [215.0768,  87.5848, 377.7890, 304.5078],\n          [179.6967,  97.9064, 277.3256, 318.6008],\n          [259.0886,  49.4857, 317.3690, 246.5554],\n          [154.6061, 103.1315, 166.7477, 134.4586],\n          [165.9859, 192.5216, 242.4949, 355.0468],\n          [154.8685, 145.3562, 410.2829, 369.0000],\n          [268.4514,  99.2900, 368.9974, 132.9262],\n          [533.5045,   2.1560, 573.5058,  98.5033],\n          [131.2304, 177.0324, 249.4475, 320.1885],\n          [ 54.0834,  90.0201, 276.5156, 342.3893],\n          [179.9769,  91.0265, 215.2010, 205.1923]]),\n  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1]),\n  'scores': tensor([0.6726, 0.6688, 0.6582, 0.6470, 0.6426, 0.6338, 0.6283, 0.6199, 0.6093,\n          0.5990, 0.5961, 0.5893, 0.5877, 0.5795, 0.5762, 0.5751, 0.5655, 0.5608,\n          0.5607, 0.5492, 0.5464, 0.5458, 0.5455, 0.5454, 0.5444, 0.5432, 0.5424,\n          0.5379, 0.5378, 0.5365, 0.5327, 0.5314, 0.5270, 0.5261, 0.5259, 0.5258,\n          0.5224, 0.5206, 0.5203, 0.5197, 0.5183, 0.5149, 0.5146, 0.5135, 0.5123,\n          0.5119, 0.5112, 0.5101, 0.5092, 0.5092, 0.5084, 0.5073, 0.5062, 0.5036,\n          0.5028, 0.5015, 0.5008, 0.4992, 0.4989, 0.4984, 0.4971, 0.4961, 0.4959,\n          0.4959, 0.4953, 0.4951, 0.4950, 0.4926, 0.4915, 0.4889, 0.4887, 0.4880,\n          0.4859, 0.4856, 0.4851, 0.4846, 0.4846, 0.4836, 0.4826, 0.4826, 0.4825,\n          0.4819, 0.4816, 0.4814, 0.4813, 0.4808, 0.4805, 0.4802, 0.4797, 0.4797,\n          0.4788, 0.4784, 0.4782, 0.4781, 0.4778, 0.4773, 0.4766, 0.4757, 0.4754,\n          0.4753])},\n {'boxes': tensor([[2.8928e+02, 1.3577e+02, 3.1601e+02, 1.9221e+02],\n          [1.4241e+02, 1.3875e+02, 1.6310e+02, 2.0570e+02],\n          [1.4974e+02, 1.2949e+02, 1.7409e+02, 2.0323e+02],\n          [1.6586e+02, 1.3139e+02, 1.8388e+02, 1.8346e+02],\n          [2.8245e+02, 1.6526e+02, 3.2976e+02, 1.9143e+02],\n          [1.0144e+02, 2.4647e+02, 1.1493e+02, 2.6880e+02],\n          [1.0832e+02, 2.5943e+02, 1.1861e+02, 2.7156e+02],\n          [2.8356e+02, 1.2707e+02, 3.0222e+02, 1.8757e+02],\n          [1.6227e+02, 1.2714e+02, 1.8078e+02, 1.6776e+02],\n          [1.1029e+02, 2.5795e+02, 1.2255e+02, 2.7385e+02],\n          [2.2640e-01, 7.8240e+01, 3.8679e+01, 1.9626e+02],\n          [1.0377e+02, 2.5322e+02, 1.1801e+02, 2.7399e+02],\n          [9.1392e+01, 1.2669e+02, 1.6935e+02, 3.4364e+02],\n          [2.5025e+02, 1.1488e+02, 3.1944e+02, 3.1354e+02],\n          [1.0392e+02, 2.6109e+02, 1.2213e+02, 2.7006e+02],\n          [3.0849e+02, 1.3326e+02, 3.3204e+02, 1.7425e+02],\n          [1.1104e+02, 2.4614e+02, 1.2194e+02, 2.6912e+02],\n          [2.8734e+02, 1.3344e+02, 3.3558e+02, 2.0614e+02],\n          [3.7916e+02, 1.2666e+02, 4.4160e+02, 3.4068e+02],\n          [5.3026e+00, 1.3465e+02, 5.0192e+01, 2.1141e+02],\n          [3.1541e+02, 1.3362e+02, 3.3892e+02, 1.9430e+02],\n          [2.5924e+02, 1.2739e+02, 3.1588e+02, 1.8451e+02],\n          [1.0545e+02, 2.6532e+02, 1.2162e+02, 2.7418e+02],\n          [2.5471e+02, 1.7770e+02, 2.7413e+02, 2.2573e+02],\n          [2.0915e+02, 1.1050e+02, 2.2877e+02, 1.6173e+02],\n          [2.8364e+02, 1.5852e+02, 3.7020e+02, 1.9565e+02],\n          [2.5780e+02, 1.3289e+02, 3.0085e+02, 2.1132e+02],\n          [2.7540e+02, 1.5733e+02, 3.4185e+02, 2.1404e+02],\n          [2.6130e+02, 1.3102e+02, 2.9224e+02, 1.7535e+02],\n          [1.3822e+02, 1.3786e+02, 1.8586e+02, 2.4483e+02],\n          [2.2023e+02, 1.2324e+02, 2.4106e+02, 1.8663e+02],\n          [1.0462e+02, 2.5523e+02, 1.2223e+02, 2.6458e+02],\n          [2.1633e+02, 1.0854e+02, 2.3670e+02, 1.6220e+02],\n          [3.1035e+02, 1.4243e+02, 3.2635e+02, 1.5737e+02],\n          [3.3762e+02, 1.3157e+02, 3.5127e+02, 1.9538e+02],\n          [2.5635e+02, 1.2884e+02, 2.8493e+02, 1.5822e+02],\n          [2.9859e+02, 1.3257e+02, 3.6098e+02, 1.9944e+02],\n          [1.4569e+02, 1.1004e+02, 1.8725e+02, 1.9649e+02],\n          [1.4767e+02, 1.7202e+02, 1.8883e+02, 3.4530e+02],\n          [3.7492e+02, 1.7869e+02, 3.8689e+02, 2.4228e+02],\n          [3.7364e+02, 1.9507e+02, 3.7993e+02, 2.3810e+02],\n          [3.9059e+02, 1.5307e+02, 4.8350e+02, 1.9920e+02],\n          [4.4206e+02, 1.2991e+02, 4.8827e+02, 2.2371e+02],\n          [1.0558e+02, 2.3746e+02, 1.2154e+02, 2.6502e+02],\n          [2.6303e+02, 1.4200e+02, 3.4941e+02, 1.9390e+02],\n          [2.4436e+02, 1.0001e+02, 2.5751e+02, 1.5041e+02],\n          [3.9190e+02, 1.4164e+02, 4.8828e+02, 1.8044e+02],\n          [2.9785e+02, 1.2782e+02, 3.2830e+02, 1.5975e+02],\n          [3.9724e+02, 1.6289e+02, 4.8918e+02, 3.5510e+02],\n          [3.3360e+02, 1.3051e+02, 3.4420e+02, 1.9821e+02],\n          [2.6141e+02, 1.6981e+02, 2.8298e+02, 2.2523e+02],\n          [3.7592e+02, 1.9523e+02, 3.8321e+02, 2.3711e+02],\n          [2.8779e+01, 4.1791e+01, 7.1134e+01, 9.8677e+01],\n          [4.5525e+02, 1.1691e+02, 4.9929e+02, 2.0630e+02],\n          [2.7518e+02, 1.3485e+02, 2.9526e+02, 1.8953e+02],\n          [4.1024e+02, 1.4039e+02, 4.5491e+02, 2.1963e+02],\n          [2.0558e+02, 1.3737e+02, 2.2258e+02, 1.5914e+02],\n          [2.0454e+02, 1.4255e+02, 2.1747e+02, 1.6456e+02],\n          [1.6088e+00, 7.7808e+01, 4.7623e+01, 1.4546e+02],\n          [3.4490e+01, 9.6485e+01, 7.7970e+01, 1.7408e+02],\n          [3.2672e+02, 1.4227e+02, 3.4395e+02, 2.0490e+02],\n          [2.5164e+02, 1.6451e+02, 3.4333e+02, 3.6171e+02],\n          [2.5471e+02, 1.5305e+02, 2.8645e+02, 2.5525e+02],\n          [2.1352e+02, 1.2340e+02, 2.2519e+02, 1.5846e+02],\n          [1.5639e+02, 1.2830e+02, 1.9795e+02, 2.6769e+02],\n          [3.1694e+01, 5.2442e+01, 8.0496e+01, 8.3462e+01],\n          [2.6820e+02, 1.2015e+02, 2.8780e+02, 1.2839e+02],\n          [1.0144e+02, 1.1991e+02, 1.4051e+02, 2.4871e+02],\n          [1.6386e+02, 1.4074e+02, 2.0928e+02, 2.0599e+02],\n          [2.1916e+02, 1.2523e+02, 2.2761e+02, 1.5955e+02],\n          [3.7153e+02, 6.6453e+01, 4.5772e+02, 2.6606e+02],\n          [2.0096e+02, 1.0331e+02, 2.4139e+02, 1.8464e+02],\n          [4.1443e+01, 1.0746e+02, 8.2816e+01, 1.3137e+02],\n          [1.1475e+02, 9.6060e+01, 1.2208e+02, 1.1069e+02],\n          [1.1546e+02, 1.2288e+02, 1.6426e+02, 2.2392e+02],\n          [1.2610e+02, 3.3656e+02, 3.3138e+02, 3.5873e+02],\n          [2.2927e+02, 1.1841e+02, 2.5359e+02, 1.8722e+02],\n          [2.6690e+02, 1.0124e+02, 3.6442e+02, 2.2442e+02],\n          [1.2517e+02, 1.4414e+02, 2.1101e+02, 3.6475e+02],\n          [4.4957e+01, 1.3799e+02, 7.7487e+01, 1.9356e+02],\n          [3.7976e+02, 1.9650e+02, 3.8632e+02, 2.3670e+02],\n          [2.6858e+02, 1.3244e+02, 3.1460e+02, 2.3478e+02],\n          [1.7203e+02, 1.0581e+02, 2.0918e+02, 2.2190e+02],\n          [2.5071e+02, 1.0800e+02, 2.5863e+02, 1.2347e+02],\n          [3.3248e+02, 1.5980e+02, 3.4098e+02, 1.9012e+02],\n          [1.8131e+02, 1.1353e+02, 2.6240e+02, 1.6232e+02],\n          [2.5976e+02, 1.6661e+02, 3.6202e+02, 2.0373e+02],\n          [2.2058e+02, 1.1554e+02, 2.2952e+02, 1.5561e+02],\n          [2.0539e+02, 1.3072e+02, 2.6721e+02, 1.9486e+02],\n          [7.4902e+01, 2.6416e+02, 5.2351e+02, 3.8121e+02],\n          [2.2617e+02, 1.0815e+02, 2.5114e+02, 1.5685e+02],\n          [2.0549e+02, 1.2636e+02, 2.2459e+02, 1.6614e+02],\n          [3.8903e+02, 1.2894e+02, 4.9480e+02, 2.8018e+02],\n          [3.9428e+02, 1.9864e+01, 5.3860e+02, 3.2583e+02],\n          [2.0025e+02, 1.2890e+02, 3.9397e+02, 3.6586e+02],\n          [2.6774e+02, 1.2322e+02, 3.5736e+02, 1.7613e+02],\n          [1.9191e+02, 1.0774e+02, 2.0146e+02, 1.3450e+02],\n          [9.8765e+01, 2.2245e+02, 1.1961e+02, 2.6187e+02],\n          [0.0000e+00, 1.5163e+02, 3.6612e+02, 3.1651e+02],\n          [3.8509e+02, 2.0231e+02, 4.0398e+02, 2.1856e+02]]),\n  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1]),\n  'scores': tensor([0.7417, 0.7021, 0.6750, 0.6628, 0.6575, 0.6567, 0.6547, 0.6472, 0.6264,\n          0.6198, 0.6176, 0.6134, 0.6123, 0.6094, 0.6088, 0.6087, 0.6078, 0.6016,\n          0.5998, 0.5993, 0.5966, 0.5957, 0.5955, 0.5941, 0.5909, 0.5901, 0.5901,\n          0.5893, 0.5879, 0.5862, 0.5862, 0.5852, 0.5838, 0.5819, 0.5817, 0.5816,\n          0.5807, 0.5773, 0.5766, 0.5711, 0.5705, 0.5698, 0.5692, 0.5678, 0.5664,\n          0.5638, 0.5617, 0.5604, 0.5596, 0.5594, 0.5585, 0.5573, 0.5549, 0.5534,\n          0.5534, 0.5522, 0.5490, 0.5470, 0.5455, 0.5426, 0.5423, 0.5413, 0.5384,\n          0.5372, 0.5363, 0.5347, 0.5334, 0.5330, 0.5321, 0.5315, 0.5313, 0.5301,\n          0.5287, 0.5286, 0.5286, 0.5286, 0.5281, 0.5256, 0.5250, 0.5237, 0.5205,\n          0.5204, 0.5200, 0.5192, 0.5184, 0.5184, 0.5183, 0.5180, 0.5177, 0.5170,\n          0.5164, 0.5152, 0.5141, 0.5140, 0.5131, 0.5121, 0.5097, 0.5086, 0.5081,\n          0.5076])}]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inspect the image and the predicted segmentation masks.\n",
    "\n",
    "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "im = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "im"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "names = [\"Background\", \"Pedestrian\"]\n",
    "im = Image.fromarray(img2.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "boxes = prediction[1]['boxes'].cpu().numpy()\n",
    "scores = prediction[1]['scores'].cpu().numpy()\n",
    "labels = prediction[1]['labels'].cpu().numpy()\n",
    "names = [names[label] for label in labels]\n",
    "im_draw = ImageDraw.Draw(im)\n",
    "for box, score, name in zip(boxes, scores, names):\n",
    "  if score > 0.5:\n",
    "    im_draw.rectangle(box)\n",
    "    corner = tuple(box[:2]  + np.asarray([0, -10]))\n",
    "    im_draw.text(corner, f\"{name}: {score}\")\n",
    "im.show()\n",
    "\n",
    "# Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "(179.97691345214844, 101.0264892578125)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(box[:2]  + np.asarray([0, 10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks pretty good!\n",
    "\n",
    "## Wrapping up\n",
    "\n",
    "In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset.\n",
    "For that, you wrote a `torch.utils.data.Dataset` class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.\n",
    "\n",
    "For a more complete example, which includes multi-machine / multi-gpu training, check `references/detection/train.py`, which is present in the [torchvision GitHub repo](https://github.com/pytorch/vision/tree/v0.3.0/references/detection).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}